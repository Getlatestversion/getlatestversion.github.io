<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on GetLatestVersion – Europe</title><link>http://www.getlatestversion.eu/</link><description>Recent content in Home on GetLatestVersion – Europe</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 03 Nov 2021 03:00:00 +0200</lastBuildDate><atom:link href="http://www.getlatestversion.eu/index.xml" rel="self" type="application/rss+xml"/><item><title>The limit is your own imagination, do it with Azure DevOps Services REST API</title><link>http://www.getlatestversion.eu/2021/11/the-limit-is-your-own-imagination-do-it-with-azure-devops-services-rest-api/</link><pubDate>Wed, 03 Nov 2021 03:00:00 +0200</pubDate><author>Bartolomeo Lombardi</author><author/><guid>http://www.getlatestversion.eu/2021/11/the-limit-is-your-own-imagination-do-it-with-azure-devops-services-rest-api/</guid><description>&lt;p>Azure DevOps is the platform signed by Microsoft that helps software factories to improve the development process, automatic tests and frequent releases. Their aim is to have additional value for the end customer in terms of product quality.&lt;/p>
&lt;p>With Azure DevOps Release you can separate the delivery environments into groups, obtaining integration, acceptance, production environments and so on. Most of time, the pipelines are configured to release the artifacts related to the main branches. However, Azure DevOps doesn&amp;rsquo;t allow to modify easily a specific release for a single target within a Deployment Group (DG) starting from different artifacts.&lt;/p>
&lt;h1 id="automated-through-the-azure-devops-services-rest-api">Automated through the Azure DevOps Services REST API&lt;/h1>
&lt;p>Consider a scenario in which we have a DG called &amp;ldquo;integration&amp;rdquo; that collects 20 virtual machines (VMs), one for each developer. Through this DG we can use a pipeline that deploys the latest version of the product to all the VMs at the same time. Anyway, more often a developer wants to test a release with his own changes before merging on the main branch. He needs to deploy his build to only his VM, without involving the others. It&amp;rsquo;s onerous for him unless he&amp;rsquo;s an Azure DevOps user, capable of modifying his deployment group, the branch where to download the artifacts, etc. It&amp;rsquo;s possible to automate all the previous steps needed to launch a release only on your target environment, starting from a particular artifact.&lt;/p>
&lt;p>In the following the steps to achive it:&lt;/p>
&lt;ol>
&lt;li>set a new tag to the VM in the DG&lt;/li>
&lt;li>put the tag into release pipeline&lt;/li>
&lt;li>launch the release pipeline&lt;/li>
&lt;li>remove the tag from DG and release pipeline&lt;/li>
&lt;/ol>
&lt;p>We have taken advantage of the &lt;a href="https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1">Azure DevOps Services REST API&lt;/a> made available by Azure DevOps through a script in PowerShell.
Here there are the essential steps:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-powershell" data-lang="powershell">Write-Host &lt;span style="color:#e6db74">&amp;#34;Adding tag for your VM in the Deployment Group: $deploymentGroup&amp;#34;&lt;/span>
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/distributedtask/deploymentgroups/&amp;#34;&lt;/span> + $DeploymentGroupId + &lt;span style="color:#e6db74">&amp;#34;/targets?api-version=6.0-preview.1&amp;#34;&lt;/span>
$output = Invoke-RestMethod -Uri $UriOrga -Method get -Headers $AzureDevOpsAuthenicationHeader
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/distributedtask/deploymentgroups/&amp;#34;&lt;/span> + $DeploymentGroupId + &lt;span style="color:#e6db74">&amp;#34;/targets?api-version=6.0-preview.1&amp;#34;&lt;/span>
Invoke-RestMethod -Method Patch -Uri $UriOrga -Headers $AzureDevOpsAuthenicationHeader -Body $TargetVMs -ContentType application/json
Write-Host &lt;span style="color:#e6db74">&amp;#34;Adding tag to Deployment Group stage in the Realese Pipeline&amp;#34;&lt;/span>
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://vsrm.dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/release/definitions/&amp;#34;&lt;/span> + $ReleaseDefinitionId + &lt;span style="color:#e6db74">&amp;#34;?api-version=6.0&amp;#34;&lt;/span>
$UpdateEnvironmentsBody = Invoke-RestMethod -Uri $UriOrga -Method Get -Headers $AzureDevOpsAuthenicationHeader -ContentType application/json
$UpdateEnvironmentsBody.environments[0].deployPhases[0].deploymentInput.tags = @( $Tag )
$UpdateEnvironmentsBody = ConvertTo-Json $UpdateEnvironmentsBody -Depth 10
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://vsrm.dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/release/definitions?api-version=6.0&amp;#34;&lt;/span>
Invoke-RestMethod -Uri $UriOrga -Method Put -Headers $AzureDevOpsAuthenicationHeader -Body $UpdateEnvironmentsBody -ContentType application/json
Write-Host &lt;span style="color:#e6db74">&amp;#34;Launching Release Pipeline for your VM&amp;#34;&lt;/span>
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://vsrm.dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/release/releases?api-version=6.0&amp;#34;&lt;/span>
$CreateReleaseBody = ConvertTo-Json $CreateReleaseBody -Depth 10
Invoke-RestMethod -Uri $UriOrga -Method Post -Headers $AzureDevOpsAuthenicationHeader -Body $CreateReleaseBody -ContentType application/json
Write-Host &lt;span style="color:#e6db74">&amp;#34;Removing all tags from Deployment Group stage in the Realese Pipeline&amp;#34;&lt;/span>
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://vsrm.dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/release/definitions/&amp;#34;&lt;/span> + $ReleaseDefinitionId + &lt;span style="color:#e6db74">&amp;#34;?api-version=6.0&amp;#34;&lt;/span>
$UpdateEnvironmentsBody = Invoke-RestMethod -Uri $UriOrga -Method Get -Headers $AzureDevOpsAuthenicationHeader -ContentType application/json
$UpdateEnvironmentsBody.environments[0].deployPhases[0].deploymentInput.tags = @()
$UpdateEnvironmentsBody = ConvertTo-Json $UpdateEnvironmentsBody -Depth 10
$UriOrga = &lt;span style="color:#e6db74">&amp;#34;https://vsrm.dev.azure.com/&amp;lt;organization&amp;gt;/&amp;lt;project&amp;gt;/_apis/release/definitions?api-version=6.0&amp;#34;&lt;/span>
Invoke-RestMethod -Uri $UriOrga -Method Put -Headers $AzureDevOpsAuthenicationHeader -Body $UpdateEnvironmentsBody -ContentType application/json
&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Like any Azure service, Microsoft&amp;rsquo;s DevOps counterpart can also be managed by a set of APIs that replicate everything that can be done through the portal. First you need to get a &lt;a href="https://docs.microsoft.com/it-it/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;amp;tabs=preview-page">PAT (Personal Access Token)&lt;/a> through which you can authenticate inside your organization.
Then you can take advantage of any tool that invokes API Rest to do whatever you want, i.e. launching Release, creating or modifying Deployment Groups, starting specific Tests and so on. This can be a hint to create a user-friendly interface for developers not accustomed to DevOps. It could automate set of actions that would be otherwise onerous.&lt;/p>
&lt;p>In these cases the limit is your own imagination.&lt;/p></description></item><item><title>Migrating work items from Jira to Azure DevOps</title><link>http://www.getlatestversion.eu/2020/08/migrating-work-items-from-jira-to-azure-devops/</link><pubDate>Tue, 18 Aug 2020 17:30:00 +0200</pubDate><author>Alessandro Alpi</author><guid>http://www.getlatestversion.eu/2020/08/migrating-work-items-from-jira-to-azure-devops/</guid><description>&lt;h1 id="intro">Intro&lt;/h1>
&lt;p>Migrations are hard tasks to deal with. Not just for IT. Working with the culture of many companies, I&amp;rsquo;ve got confirmation that the tool should be considered at the end of the migration process. After setting up the ceremonies and switching the methodologies from legacy to lean/iterative, it comes finally to choose from the available tools (&lt;em>enterprise awareness&lt;/em>) and including new ones. The goal is to get all the stuff which fits the real scenario.&lt;/p>
&lt;p>This post is a quick step by step guide to migrate work items from &lt;a href="https://www.atlassian.com/software/jira">Jira cloud&lt;/a> to &lt;a href="https://azure.microsoft.com/it-it/services/devops/">Azure DevOps Services&lt;/a>. I&amp;rsquo;m going to describe the last step of one of my customers&amp;rsquo; migration.&lt;/p>
&lt;h2 id="table-of-content">Table of content&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#Getting-started">Getting started&lt;/a>&lt;/li>
&lt;li>&lt;a href="#The-software-selection">The software selection&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Executing-the-tool">Executing the tool&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#How-to-export-Jira-issues">How to export Jira issues&lt;/a>&lt;/li>
&lt;li>&lt;a href="#How-to-import-to-Azure-DevOps">How to import to Azure DevOps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#Conclusions">Conclusions&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>Before going in-depth with technical details, I would like to share some tips. As we have already said, the migrations are complex tasks. Mandatory requirements should be a strong knowledge in terms of business and team management, enterprise awareness and years of experience on different real scenarios.&lt;/p>
&lt;p>We must avoid any decision if our ideas and targets are not clear. Another important requirement is to understand in depth the workflows you will work on, both the legacy one and the new one you&amp;rsquo;re figuring out. Some of the question we should ask ourselves are:&lt;/p>
&lt;ul>
&lt;li>Do we require these steps? And what about these work items?&lt;/li>
&lt;li>Is this state workflow good for us? Should we change it? How?&lt;/li>
&lt;li>Do we require to preserve relationships and items&amp;rsquo; history?&lt;/li>
&lt;li>Can we keep something which is working very well now? If so, what tools we&amp;rsquo;re thinking about?&lt;/li>
&lt;/ul>
&lt;h2 id="the-software-selection">The software selection&lt;/h2>
&lt;p>The software selection has ended up on a tool made by &lt;a href="https://solidify.se/">Solidify&lt;/a> (Thanks to the experienced members of our &lt;a href="https://www.getlatestversion.eu/">getlatestversion.eu&lt;/a> community). Anyways, you can find more of them. For example:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ilclubdellesei.blog/2018/05/21/import-from-jira-to-vsts-in-5-steps/">TFS4JIRA&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.codewrecks.com/blog/index.php/2019/01/19/import-work-item-from-external-system-to-azure-devops/">Importing work items to Azure DevOps by Gian Maria Ricci&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ilclubdellesei.blog/2018/05/21/import-from-jira-to-vsts-in-5-steps/">JiraToVsts &lt;/a> (via &lt;a href="https://www.getlatestversion.eu/it/authors/phenix/">Michele Ferracin&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>When exporting from Jira, the CLI implemented by &lt;a href="https://solidify.se/">Solidify&lt;/a> connects to the Jira server (cloud by default and on-premises when configured), executes a &lt;a href="https://www.atlassian.com/software/jira/guides/expand-jira/jql">JQL&lt;/a> query for extracting the Jira so-called &amp;ldquo;Issues&amp;rdquo;, evaluates and applies the mapping between users, work items, relationships and states, and exports a JSON file for each item.&lt;/p>
&lt;p>When importing to Azure DevOps, the CLI imports the output JSON files using the same mapping configured into the configuration file in the exporting phase.&lt;/p>
&lt;p>Why this tool? Because it has a couple of simple command lines and it consumes a JSON configuration which is clear. Additionally, it has many default behaviours, like the built-in configuration for &lt;em>Scrum&lt;/em>, &lt;em>agile&lt;/em> and &lt;em>basic&lt;/em> process templates, which allows us to forget about the complexity of both the source and target software.&lt;/p>
&lt;p>&lt;a href="#Intro">Back to top&lt;/a>&lt;/p>
&lt;h2 id="executing-the-tool">Executing the tool&lt;/h2>
&lt;p>The scenario (Jira) has been configured with many states, sometimes with overlapping meaning (due to the legacy setup and different team&amp;rsquo;s approach) and with custom workflows/links. On the other hand, Azure DevOps side, I&amp;rsquo;ve created a customized Scrum template, with just two new work item types, which should support some of the customized source behaviours, and a couple of new states. So the tool has been configured as depicted in the following JSON (just a subset of maps):&lt;/p>
&lt;p>&lt;img src="./01-config.png" alt="config">&lt;/p>
&lt;p>Just that. Notice that we can configure project names, the JQL query for gathering issues, working folder names and the file for the user mappings.&lt;/p>
&lt;p>First, download &lt;a href="https://github.com/solidify/jira-azuredevops-migrator/releases">the latest release of the CLI from GitHub&lt;/a>. Then follow these steps-&lt;/p>
&lt;h3 id="how-to-export-jira-issues">How to export Jira issues&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>create a folder called C:/Temp/JiraExport (you can configure this editing the JSON config file)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>create a file called &amp;ldquo;users.txt&amp;rdquo; within that folder and put into it a list of &lt;code>jirauser@domain.ext=AzDOs@domain.ext&lt;/code> records&lt;/p>
&lt;ul>
&lt;li>please note that the Jira user can be represented without the domain, depending on its configuration&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>copy the JSON configuration file (based on the template we&amp;rsquo;re going to apply when importing) into the JiraExport folder&lt;/p>
&lt;ul>
&lt;li>modify the file in the maps section: link-map, type-map, field-map, and so on.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>get the credentials (as admin) and the Jira server URL&lt;/p>
&lt;/li>
&lt;li>
&lt;p>make your command line, which should look like the following:
&lt;code>jira-export -u username@domain.ext -p userPwd --url https://jiraurl.ext --config config-scrum.json --force&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>run the command and look for the JSON files into the JiraExport folder&lt;/p>
&lt;ul>
&lt;li>look for any warning/error like the following and correct them (this will help you to import without any breaking change)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="./02-errors.png" alt="errors">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;a href="#Intro">Back to top&lt;/a>&lt;/p>
&lt;h3 id="how-to-import-to-azure-devops">How to import to Azure DevOps&lt;/h3>
&lt;p>It&amp;rsquo;s time to execute the second command line, &lt;strong>wi-import&lt;/strong>. As we can see, we have to get a personal access token (PAT) from Azure DevOps, as described &lt;a href="https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;amp;tabs=preview-page">in the documentation&lt;/a>, with the full access.&lt;/p>
&lt;p>Coming back to the configuration, we should pay attention to &lt;code>base-area-path&lt;/code> and &lt;code>base-iteration-path&lt;/code>. With these, we can choose the target of our migration, in terms of area and iteration. This means that we can manage our items after the import has been completed. With a query, for example, we can remove everything and start over with the migration. Cool. The command should like the following:&lt;/p>
&lt;p>&lt;code>wi-import --token PAT --url https://dev.azure.com/org --config config-scrum.json --force&lt;/code>&lt;/p>
&lt;p>&lt;img src="./03-import.png" alt="import">&lt;/p>
&lt;p>After ten minutes we&amp;rsquo;ve got new areas and iterations:&lt;/p>
&lt;p>&lt;img src="./04-iterations.png" alt="iterations"> &lt;img src="./05-areas.png" alt="areas">&lt;/p>
&lt;p>The Azure DevOps hierarchy of items (except for the Features, which I&amp;rsquo;ve not mapped) has been preserved and also the history of any item:&lt;/p>
&lt;p>&lt;img src="./06-history.png" alt="history">&lt;/p>
&lt;p>&lt;a href="#Intro">Back to top&lt;/a>&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>In a couple of days, we&amp;rsquo;ve migrated everything related to work items from Jira to Azure DevOps. Most of the time should be invested in configuring the JSON mappings and, for sure, to check for the errors caught during exporting items. Finally, after importing into AzDOs, you can start over and over, removing all the items under the pre-configured area/iteration, until everything looks good.&lt;/p>
&lt;p>&lt;a href="#Intro">Back to top&lt;/a>&lt;/p></description></item><item><title>Git Cherry-Pick for a strategic harvest</title><link>http://www.getlatestversion.eu/2020/07/git-cherry-pick-for-a-strategic-harvest/</link><pubDate>Mon, 13 Jul 2020 03:00:00 +0200</pubDate><author>Bartolomeo Lombardi</author><guid>http://www.getlatestversion.eu/2020/07/git-cherry-pick-for-a-strategic-harvest/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>IT companies which develop and sell software product are always focused on delivering new features to propose new versions of them to the final client, while still considering the maintenance of the previous release.
Generally, we are talking about projects of millions of lines of code that requires multiple developing teams located around the world with strong coordination and collaboration among them, in which it is necessary to handle multiple versions of the same product with a significant number of release branches. Fixing a bug on multiple branches is a complex task because the developer needs to backport the same fix in the various release branches possible with Git and it is called &amp;ldquo;cherry-picking&amp;rdquo;.&lt;/p>
&lt;h1 id="git-cherry-pick">Git cherry-pick&lt;/h1>
&lt;p>One of the most powerful Git commands is Cherry-pick. It takes one or more commits as input parameters and shifts them to a different branch, by creating a new commit in the process.&lt;/p>
&lt;p>Cherry-pick is commonly used in many Git workflows such as the Azure DevOps team’s, described in the following &lt;a href="https://devblogs.microsoft.com/devops/improving-azure-devops-cherry-picking/">article&lt;/a>, see the image reported below for a visual representation of the workflow.
&lt;img src="cherry-pick-workflow.jpg" alt="Cherry-Pick: way of working">&lt;/p>
&lt;p>Every time a developer works with many version of the same product it is essential to ensure all the reported bugs have been fixed in as many version as possible. Cherry-picking happens on the main branch to avoid deploying a new release with the same bug.&lt;/p>
&lt;h2 id="azure-devops-repos">Azure DevOps Repos&lt;/h2>
&lt;p>Azure DevOps provides cherry-picking of a completed Pull Request (PR) or of a single commit by clicking a dedicated button. The process will create a new PR with the same fix.
&lt;img src="azdo-cp.jpg" alt="Cherry-Pick Azure DevOps">&lt;/p>
&lt;p>Moreover, a &lt;a href="https://github.com/microsoft/azure-repos-pr-multi-cherry-pick">PR Multi-Cherry-Pick&lt;/a> is possible by means of an open source extension available on Azure DevOps Marketplace.&lt;/p>
&lt;p>If Azure DevOps displays a warning about the cherry-picking not being performed automatically (see image below) this is related to conflicts generated during the merge, therefore it has to be performed locally.
&lt;img src="azdo-cp-error.jpg" alt="Azure DevOps conflict errors">&lt;/p>
&lt;p>Many development environments integrated with Git can perform this local operation by graphic user interface. Best candidates are Visual Studio or VS Code.&lt;/p>
&lt;h1 id="git-cherry-pick---continue">Git cherry-pick &amp;ndash;continue&lt;/h1>
&lt;p>A few steps are needed to execute the following command &lt;a href="https://git-scm.com/docs/git-cherry-pick">git cherry-pick&lt;/a>.&lt;/p>
&lt;p>The first step is noting of the hash commit value to cherry-pick. If within a PR, the list of commits is available in the &lt;strong>Commits&lt;/strong> tab in Azure Repos:
&lt;img src="azdo-commits-tab.jpg" alt="Commit table on Azure DevOps">&lt;/p>
&lt;p>Once you are on the branch (&lt;code>git checkout &amp;lt;branch-name&amp;gt;&lt;/code>) that you want to commit, the following command must be executed &lt;code>git cherry-pick &amp;lt;commit&amp;gt;&lt;/code>.&lt;/p>
&lt;p>Knowing that the cherry-pick command shown in the previous example can generate another conflict, once the code is merged manually you need to run the &lt;code>git cherry-pick --continue&lt;/code> to proceed. If you want to abort the process simply run git &lt;code>cherry-pick --abort&lt;/code>.&lt;/p>
&lt;p>All you need to do now is to perform &lt;code>git push&lt;/code> to align the remote.&lt;/p></description></item><item><title>Flaky builds</title><link>http://www.getlatestversion.eu/2020/06/flaky-builds/</link><pubDate>Sun, 07 Jun 2020 20:00:00 +0100</pubDate><author>Giulio Vian</author><guid>http://www.getlatestversion.eu/2020/06/flaky-builds/</guid><description>&lt;p>The topic is huge and I do not have the room to go through all the details in one post, so I stick to a cursory view. I will sprinkle references to additional material so you can deep dive later.&lt;/p>
&lt;h2 id="what-is-a-flaky-build">What is a &amp;lsquo;flaky&amp;rsquo; build?&lt;/h2>
&lt;p>A &amp;lsquo;flaky&amp;rsquo; build is a build whose outcome is, in some ways, unpredictable. This is generally bad because you expect your CI to be algorithmic, deterministic, with no randomness. The worse consequence of an unpredictable build lies in the future: that day when you run the same build, maybe in the hurry of delivering a patch, and it fails unexpectedly. Some may think that is is just another piece of evidence for Murphy’s Law&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, which applies to all time and space.
The unpredictability may be subtle; for example, the build always succeeds but its results are never quite the same.
In the following, I will review the most common flakiness scenarios.&lt;/p>
&lt;h2 id="same-source-different-binaries">Same source, different binaries&lt;/h2>
&lt;p>Have you tried comparing binaries built on two different machines? More often than not, they will be different! Even if you use the exact same source code version. Worse, even on the same machine, rebuilding the same code at a later time will produce different binaries.&lt;/p>
&lt;p>My first example is a .NET 4.x assembly: the next picture shows the binary difference between files built from the same sources a few minutes apart.
&lt;img src="dotnet4.8-hexdiff.png" alt="Binary diff of Exes">
Traditionally Microsoft tools embedded some kind of build timestamp in binaries for a number of reason, especially to help Windows&amp;rsquo; Loader&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>I obtained a similar result with Maven building a JAR package, I used Beyond Compare to compare more easily.
&lt;img src="mvn-diff1.png" alt="JAR content difference">
The difference is in the &lt;code>pom.properties&lt;/code> manifest, which contains a time-stamp marking when the package was built.
&lt;img src="mvn-diff2.png" alt="Maven manifest difference">&lt;/p>
&lt;p>There are ways to fix these behaviors and join the camp of repeatable builds, like myself.&lt;/p>
&lt;h3 id="why-should-i-care">Why should I care?&lt;/h3>
&lt;p>The ability to compare binaries helps or is even required in many circumstances:&lt;/p>
&lt;ul>
&lt;li>troubleshooting, you can quickly compare a binary file generated on your machine with another from unknown source;&lt;/li>
&lt;li>deployment, you can skip copying/deploying binary identical files;&lt;/li>
&lt;li>patching, generating binary patches is easier;&lt;/li>
&lt;li>auditing, you can demonstrate the version of code that is running in production matches source code;&lt;/li>
&lt;li>security, the binary hasn&amp;rsquo;t been tampered by an attacker.&lt;/li>
&lt;/ul>
&lt;p>Hope this convince you that these goals are worth your time and effort. Auditing and security are becoming of greater importance these days.&lt;/p>
&lt;h3 id="reproducible-builds">Reproducible builds&lt;/h3>
&lt;p>Let&amp;rsquo;s state a clear goal, we want reproducible builds that generate identical &lt;em>main&lt;/em> artifacts when using the same version of source code.
What do I mean with &lt;em>main&lt;/em> artifacts? That we allow for ancillary artifacts to change at every run; a trivial example is a build log or release note file. We forbid differences in executable binaries, data or configuration files, scripts and tools.
With this definition, a release note file displaying a build id or timestamp is fine.&lt;/p>
&lt;p>To achieve this goal, you have to look at the tools&amp;rsquo; manuals. I will detail a few common scenarios.
In addition to the following suggestions, you have to consider a proper archival and versioning of the toolchain in use to build the software. A newer version of compiler, SDK, build may produce different results.&lt;/p>
&lt;h3 id="net-reproducible-builds">.NET Reproducible builds&lt;/h3>
&lt;p>The Roslyn C# compiler offers the &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/compiler-options/deterministic-compiler-option">&lt;code>-deterministic&lt;/code>&lt;/a> flag a few years now, that is, from Visual Studio 2015. You turn on the option setting the &lt;code>Deterministic&lt;/code> MSBuild property to &lt;code>true&lt;/code> in the project file.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml"> &lt;span style="color:#f92672">&amp;lt;PropertyGroup&amp;gt;&lt;/span>
&lt;span style="color:#f92672">&amp;lt;Deterministic&amp;gt;&lt;/span>true&lt;span style="color:#f92672">&amp;lt;/Deterministic&amp;gt;&lt;/span>
&lt;span style="color:#f92672">&amp;lt;PathMap&amp;gt;&lt;/span>$(MSBuildThisFileDirectory)=\src&lt;span style="color:#f92672">&amp;lt;/PathMap&amp;gt;&lt;/span>
&lt;span style="color:#f92672">&amp;lt;/PropertyGroup&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>By the way, you do not need to rush editing you project files to set these properties. You can add a &lt;code>Directory.Build.props&lt;/code> file&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> at the root of your project and the setting will be applied to all projects under the same directory.
I have a good news: .NET Core SDK projects use the &lt;code>deterministic&lt;/code> flag by default. The bad news is that it is not enough in some cases, but keep on reading.&lt;/p>
&lt;p>In the above example, you have surely noted another property, &lt;code>PathMap&lt;/code>, which matches the &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/compiler-options/pathmap-compiler-option">&lt;code>-pathmap&lt;/code>&lt;/a> compiler option. This option alters the paths embedded in executables and PDBs&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> and is especially useful when building on a server. Typically each agent on the server has a different base directory, and the generated files embed the agent specific path. With &lt;code>PathMap&lt;/code> you define a conventional directory (&lt;code>\src&lt;/code> in the example) independent from the effective directory used for the build.
A useful resource is Jared Parsons&amp;rsquo; post &lt;a href="https://blog.paranoidcoding.com/2016/04/05/deterministic-builds-in-roslyn.html">&lt;em>Deterministic builds in Roslyn&lt;/em>&lt;/a>.&lt;/p>
&lt;h3 id="maven-java-reproducible-builds">Maven (Java) Reproducible builds&lt;/h3>
&lt;p>It is very easy to replace the timestamp in &lt;code>pom.properties&lt;/code>, just add a &lt;code>project.build.outputTimestamp&lt;/code> property&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml"> &lt;span style="color:#f92672">&amp;lt;properties&amp;gt;&lt;/span>
&lt;span style="color:#f92672">&amp;lt;project.build.outputTimestamp&amp;gt;&lt;/span>2020-06-06T22:12:34Z&lt;span style="color:#f92672">&amp;lt;/project.build.outputTimestamp&amp;gt;&lt;/span>
&lt;span style="color:#f92672">&amp;lt;/properties&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>to the &lt;code>pom.xml&lt;/code> file. This requires that you use a recent version (3.2 or later) of Maven plugins. You can find all the details in &lt;a href="https://maven.apache.org/guides/mini/guide-reproducible-builds.html">&lt;em>Configuring for Reproducible Builds&lt;/em>&lt;/a>.&lt;/p>
&lt;p>The &lt;code>pom.properties&lt;/code> solves the simple issue but there are additional data, used during build, that can produce different outcomes. A more through approach use the &lt;a href="https://zlika.github.io/reproducible-build-maven-plugin/">Reproducible Build Maven Plugin&lt;/a> which guarantees identical binaries given unchanged sources.&lt;/p>
&lt;h3 id="other-languages">Other languages&lt;/h3>
&lt;p>C/C++ developers can profitably study &lt;a href="https://blog.conan.io/2019/09/02/Deterministic-builds-with-C-C++.html">An introduction to deterministic builds with C/C++&lt;/a>; this article explores the main issue on each major platform (Windows, Mac, and Linux).
I recommend the &lt;a href="https://reproducible-builds.org/">https://reproducible-builds.org/&lt;/a> site for Linux and C/C++ information about Reproducible builds.
C/C++ reproducible can be complex to implement, caring for lots of detail, so I do not even dare to start in a short article like this one.&lt;/p>
&lt;p>Go language has its quirks too. You may want to avoid the C compiler and linker, using the &lt;code>CGO_ENABLED=0&lt;/code> setting, or embrace complexity. The other main issue is path strings embedded in binaries. The &lt;a href="https://golang.org/cmd/go/#hdr-Compile_packages_and_dependencies">&lt;code>-trimpath&lt;/code>&lt;/a> flag, available with Go 1.13 and later, resolves using a conventional path (similar to C# &lt;code>-pathmap&lt;/code>).&lt;/p>
&lt;h2 id="same-definition-different-set-of-files">Same definition, different set of files&lt;/h2>
&lt;p>Dependencies are the next source of troubles for build predictability. I see this as a separate topic from reproducible builds because the focus is not the individual file you coded, but the general outcome and files outside your direct control.&lt;/p>
&lt;p>The issue caused by dependencies is simple to describe. Your project depends on, at least one, external file. The first time you build, you get a version of this external file, say v1.0. The next time you build, the version for the external file is different: it can be v1.0.1, v1.1, v2.0 or else.&lt;/p>
&lt;p>We can set apart four reasons for non-predictable outcomes when it comes to dependencies:&lt;/p>
&lt;ul>
&lt;li>Loose specification &amp;amp; newer packages versions available;&lt;/li>
&lt;li>Newer versions available for indirect dependencies;&lt;/li>
&lt;li>Loss of package source (direct or indirect);&lt;/li>
&lt;li>Change in package manager and its algorithm.&lt;/li>
&lt;/ul>
&lt;h3 id="loose-dependencies-specifications">Loose dependencies specifications&lt;/h3>
&lt;p>Every modern library manager permits to specify a strict or a loose rule for a dependent file. The strict rule states a unique version for the library package. A loose rule defines a range of acceptable versions.
When the library author publishes a new version, what happens to your build depends on the rule. A strict rule will always retrieve the same version, a loose rule may force the package manager to download a different version. Let&amp;rsquo;s see a practical example.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="center">Date&lt;/th>
&lt;th align="center">Dependency Rule&lt;/th>
&lt;th align="center">Available versions&lt;/th>
&lt;th align="center">Version selected&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="center">1 Jan&lt;/td>
&lt;td align="center">= 1.0&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Jan&lt;/td>
&lt;td align="center">1.0&amp;lt;= and &amp;lt;2.0&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Feb&lt;/td>
&lt;td align="center">= 1.0&lt;/td>
&lt;td align="center">1.0, 1.1&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Feb&lt;/td>
&lt;td align="center">1.0&amp;lt;= and &amp;lt;2.0&lt;/td>
&lt;td align="center">1.0, 1.1&lt;/td>
&lt;td align="center">1.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Mar&lt;/td>
&lt;td align="center">= 1.0&lt;/td>
&lt;td align="center">1.0, 1.1, 1.2, 2.0&lt;/td>
&lt;td align="center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Mar&lt;/td>
&lt;td align="center">1.0&amp;lt;= and &amp;lt;2.0&lt;/td>
&lt;td align="center">1.0, 1.1, 1.2, 2.0&lt;/td>
&lt;td align="center">1.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Apr&lt;/td>
&lt;td align="center">= 1.0&lt;/td>
&lt;td align="center">1.2, 2.0, 2.1&lt;/td>
&lt;td align="center">error&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">1 Apr&lt;/td>
&lt;td align="center">1.0&amp;lt;= and &amp;lt;2.0&lt;/td>
&lt;td align="center">1.2, 2.0, 2.1&lt;/td>
&lt;td align="center">1.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In this example, after the author publishes a new minor version, the build (&lt;em>rectius&lt;/em> the package manager) use the new minor version for the looser rule, while the strict rule stick to version 1.0.
You may have noticed the 7th line, the build using the strict rule fails because the required version of the package is no more available. This is not the scenario we will discuss in &lt;a href="#lost-source">&lt;em>Lost source&lt;/em>&lt;/a>, but the solution is the same.
Note that some package managers do not select the most recent version of a package (Maven is an example).&lt;/p>
&lt;h3 id="indirect-dependencies">Indirect dependencies&lt;/h3>
&lt;p>The &lt;a href="#loose-dependencies-specifications">above&lt;/a> scenario is very simple because it analyses a direct dependency, i.e. the code you are building directly depends on the specified library version.
This is rarely the case in practice: the libraries you use depends on other libraries and so on. On average a project has a couple of hundred dependencies, direct or indirect &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>While you can set very strict rules for your direct dependencies, you have no control on indirect dependencies. Only the library author can define the rules for the latter.
Let&amp;rsquo;s see a simple example.&lt;/p>
&lt;p>&lt;img src="diamond-dependency-graph.png" alt="Diamond dependency graph">&lt;/p>
&lt;p>We say that &lt;strong>Library C&lt;/strong> is a transitive dependency for App.
&lt;strong>Library A&lt;/strong> requires at least version 2.0 of &lt;strong>Library C&lt;/strong> while &lt;strong>Library C&lt;/strong> requires a minimum 3.0 version. The package manager will pick version 3, which may have unexpected effects.&lt;/p>
&lt;p>There is a solution to this and the previous issue and is generally referred as &lt;em>locking&lt;/em>.&lt;/p>
&lt;h3 id="fixing-loose-specifications-and-indirect-dependencies">Fixing loose specifications and indirect dependencies&lt;/h3>
&lt;p>The most used library package managers offer a &lt;em>locking&lt;/em> mechanism: the tool evaluates the whole dependency graph, determining the set of version that satisfy all dependency rules, save the result in a &lt;em>lock&lt;/em> file, and use such result from now on without evaluating again. This mechanism is sometimes called &lt;em>pinning&lt;/em> or &lt;em>fixing&lt;/em> library versions, as you are taking a snapshot of dependency graph at a point in time.&lt;/p>
&lt;p>&lt;strong>NuGet&lt;/strong> uses &lt;a href="https://github.com/NuGet/Home/wiki/Enable-repeatable-package-restore-using-lock-file">&lt;code>packages.lock.json&lt;/code> file&lt;/a> to control locking. The presence of the file, even empty, triggers the mechanism. You can have this file at the project level (e.g. same directory as &lt;code>.csproj&lt;/code> project file) or at the root of the Git repository, depending how you prefer managing dependencies.&lt;/p>
&lt;p>&lt;strong>Maven&lt;/strong> requires that you use the &lt;a href="http://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management">&lt;code>&amp;lt;dependencyManagement&amp;gt;&lt;/code> stanza&lt;/a> to define the versions for direct and indirect dependencies. This mechanism is not 100% robust, so the community has devise some plugins to help manage the list and detect violations like &lt;a href="https://github.com/vandmo/dependency-lock-maven-plugin">dependency-lock-maven-plugin&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Gradle&lt;/strong> offers a very sophisticated mechanism based on a set of &lt;a href="https://docs.gradle.org/current/userguide/dependency_locking.html">lock files&lt;/a>. It is an opt-in mechanism that requires modification to the &lt;code>build.gradle&lt;/code> script.&lt;/p>
&lt;p>&lt;strong>npm&lt;/strong> uses &lt;a href="https://docs.npmjs.com/configuring-npm/package-locks.html">&lt;code>package-lock.json&lt;/code>&lt;/a> file.&lt;/p>
&lt;p>Python has a few tools to solve this problem. Notable is &lt;a href="https://github.com/pypa/pipenv">pipenv&lt;/a> which uses &lt;code>Pipfile.lock&lt;/code>. In a build script, you use the &lt;code>pipenv sync&lt;/code> command which uses the &lt;code>Pipfile.lock&lt;/code> exclusively.&lt;/p>
&lt;h3 id="lock-files-and-pipelines">Lock files and pipelines&lt;/h3>
&lt;p>Once the lock file is generated, commit it to source control. This guarantees that others developers, and build pipelines use the same lock file as you.&lt;/p>
&lt;p>Be aware that the content of lock files is not written on stone. If you change the main dependency file (&lt;code>.csproj&lt;/code>, &lt;code>packages.config&lt;/code>, &lt;code>packages.json&lt;/code>, etc.), the library package manager updates the lock at the first run, unless you block this behaviour. In a build must &lt;strong>always&lt;/strong> force the use of a lock file, that is:&lt;/p>
&lt;ul>
&lt;li>set MSBuild &lt;code>RestoreLockedMode&lt;/code> property to &lt;code>true&lt;/code>;&lt;/li>
&lt;li>use &lt;code>dotnet restore&lt;/code> command with the &lt;code>--locked-mode&lt;/code> option;&lt;/li>
&lt;li>use &lt;code>npm ci&lt;/code> &lt;strong>not&lt;/strong> &lt;code>npm install&lt;/code>;&lt;/li>
&lt;li>use &lt;code>pipenv sync&lt;/code> &lt;strong>not&lt;/strong> &lt;code>pipenv install&lt;/code>;&lt;/li>
&lt;/ul>
&lt;h3 id="lost-source">Lost source&lt;/h3>
&lt;p>We still have one scenario depicted in the table: how to cope with library versions that are no more available.
If you think that it never happens, please read the &lt;a href="https://www.theregister.com/2016/03/23/npm_left_pad_chaos/">story of left-pad&lt;/a>.&lt;/p>
&lt;p>The solution is to add some kind of buffer between your build and the external world, a storage for input artifacts. There quite a number of products, generally labelled as &lt;em>Artifact Store&lt;/em> or &lt;em>Artifact Manager&lt;/em>: &lt;a href="https://jfrog.com/artifactory/">Artifactory&lt;/a>, &lt;a href="https://www.myget.org/">MyGet&lt;/a>, &lt;a href="https://www.sonatype.com/product-nexus-repository">Nexus&lt;/a>, &lt;a href="https://inedo.com/proget">ProGet&lt;/a>, &lt;a href="https://docs.microsoft.com/en-us/azure/devops/artifacts">Azure Artifacts&lt;/a> and so on.&lt;/p>
&lt;p>These products require some tweaks on the build configuration so that the library package manager asks the Artifact Manager before going to the original source. NuGet uses the &lt;a href="">&lt;code>NuGet.Config&lt;/code> file&lt;/a> at machine or user level to search for sources; npm leverages the &lt;a href="https://docs.npmjs.com/misc/config#registry">&lt;code>registry&lt;/code>&lt;/a> key in its configuration and so on.
The Artifact Manager returns the library package, if available locally, or goes to the original source and cache it locally.&lt;/p>
&lt;p>Artifact Managers offer support to the most common library package formats (Maven, npm, NuGet, etc.) and a caching mechanism that shields you from a sudden disappearance of a library.&lt;/p>
&lt;h3 id="tool-chain-issues">Tool chain issues&lt;/h3>
&lt;p>The final piece is the library package manager tool itself. Matching algorithms, syntax, all might change on newer version. It is important to have copies of the executable, its configuration.
The golden rule is being able to recreate the exact build environment that was used at any point in the past. I discussed the value of Docker in details in &lt;a href="http://blog.casavian.eu/2019/08/19/meta-pipelines-introduction/">this article&lt;/a>.&lt;/p>
&lt;h2 id="same-tests-different-results">Same tests, different results&lt;/h2>
&lt;p>Flaky tests are the third major cause of uncertainty in builds. Flaky tests randomly pass or fail despite the build being identical.&lt;/p>
&lt;p>What kind of test can be so unpredictable? Tests that depends on external resources. For example, you deploy a web application and run a smoke test that checks if the site is up; if the web application host is slow to start, the remote call may time-out and the test fails. Unit tests, by definition, do not depends on external resources, so your focus should be on integration tests. Design integration tests for predictability; it is better that a whole suite of tests fails. e.g. by checking connectivity at entering the suite, than having a random test of the suite failing. Another scenario might be that tests have an implicit order, and the runner executes them in parallel, thus the random outcome.
Martin Fowler wrote a beautiful &lt;a href="https://martinfowler.com/articles/nonDeterminism.html">essay on flaky tests&lt;/a> and I recommend studying it to gain a deep understanding why this happens and some preventions.&lt;/p>
&lt;h3 id="how-to-harness">How to harness&lt;/h3>
&lt;p>Practitioner have to face the reality of unforeseen problems. Luckily, modern build tools offer some relief.
You can automatically re-run failed tests and report which tests pass on the second (or more) attempt.&lt;/p>
&lt;ul>
&lt;li>in Azure DevOps you turn on this feature at the project level (&lt;a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/test/flaky-test-management">https://docs.microsoft.com/en-us/azure/devops/pipelines/test/flaky-test-management&lt;/a>);&lt;/li>
&lt;li>Jenkins has a plugin named &lt;a href="https://plugins.jenkins.io/flaky-test-handler/">&lt;em>Flaky Test Handler&lt;/em>&lt;/a> somewhat limited;&lt;/li>
&lt;li>GitLab suggests a &lt;a href="https://about.gitlab.com/handbook/engineering/quality/guidelines/debugging-qa-test-failures/#flaky-test">sophisticated process&lt;/a> that accounts for a number of failure scenarios, allowing tagging tests so the build pipeline knows about flakiness or other issues.&lt;/li>
&lt;/ul>
&lt;h2 id="in-summary">In summary&lt;/h2>
&lt;p>We explored cursory the topic of non-predictable builds. The root of randomness may lie in the compiler, the linker, the library manager, the dependency manager, in the test suites.
There are options, tools and techniques that help in minimise or even completely get rid of variation and have fully reproducible builds.
You should aim at repeatable, even reproducible builds: they are a defense weapon for the developer in the modern world.
A final word for the tool-chain. If you need to go back and rebuild years old versions, you must archive the exact set of tools you use for building and keep it safe. A different version for a compiler or a library manager can have a big impact on your ability to reproduce an old version.&lt;/p>
&lt;hr>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&amp;ldquo;Anything that can go wrong will go wrong&amp;rdquo;. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Raymond Chen&amp;rsquo;s post &lt;a href="https://devblogs.microsoft.com/oldnewthing/20100318-00/?p=14563">What is DLL import binding?&lt;/a> and also &lt;a href="https://devblogs.microsoft.com/oldnewthing/20180103-00/?p=97705">Why are the module timestamps in Windows 10 so nonsensical?&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>MSBuild documentation &lt;a href="https://docs.microsoft.com/en-us/visualstudio/msbuild/customize-your-build">Customize your build&lt;/a>. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Program database (PDB) is Microsoft&amp;rsquo;s file format for storing debugging information. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Source &lt;a href="https://octoverse.github.com/#dependencies-overview">GitHub&lt;/a>: &amp;ldquo;&lt;em>203 package dependencies, on average, support every public and private repository with an enabled dependency graph&lt;/em>&amp;rdquo; &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Top 10 Pipeline mistakes</title><link>http://www.getlatestversion.eu/2020/05/top-10-pipeline-mistakes/</link><pubDate>Sat, 23 May 2020 14:00:00 +0100</pubDate><author>Giulio Vian</author><guid>http://www.getlatestversion.eu/2020/05/top-10-pipeline-mistakes/</guid><description>&lt;p>Today I am going to start a series of posts detailing common issues or mistakes in a DevOps context.
I will try to refer to my experience and add some practical suggestion to identify and solve these issues.&lt;/p>
&lt;p>Let&amp;rsquo;s start with my list of top 10 CI/CD pipeline issues.&lt;/p>
&lt;h2 id="the-list">The list&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>NOTE&lt;/strong>:&lt;br>
Published posts are in &lt;strong>bold&lt;/strong>, the link for items in &lt;em>italic&lt;/em> does not work.&lt;br>
Last update: 10 June 2020&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>&lt;a href="../sloppy-secrets-handling">&lt;em>Sloppy handling of Secrets&lt;/em>&lt;/a> &amp;ndash; leaking or hard-coding passwords, tokens or similar sensitive data;&lt;/li>
&lt;li>&lt;a href="../untraceable-artifacts">&lt;em>Untraceable artifacts&lt;/em>&lt;/a> &amp;ndash; when builds produce (or worse: deploy!) binaries of unknown source and version; this is a major red flag because it is cheap and easy to fix, but it is usually overlooked causing a major technical debt pile-up;&lt;/li>
&lt;li>&lt;a href="../too-specific">&lt;em>Too specific&lt;/em>&lt;/a> &amp;ndash; if your artifacts are not scrubbed from environment-specific dependencies, so they cannot be deployed to all environments;&lt;/li>
&lt;li>&lt;a href="../what-quality">&lt;em>What, quality?&lt;/em>&lt;/a> &amp;ndash; when your pipeline does not contain any check on quality, what do you expect as a result?;&lt;/li>
&lt;li>&lt;a href="../bleeding-edge">&lt;em>Bleeding edge&lt;/em>&lt;/a> &amp;ndash; using the latest and greatest technology is not always a wise choice;&lt;/li>
&lt;li>&lt;a href="../galactic-builds">&lt;em>Galactic Builds&lt;/em>&lt;/a> &amp;ndash; far-reaching builds that slow teams down instead of helping them;&lt;/li>
&lt;li>&lt;a href="../flaky-builds">&lt;strong>Flaky builds&lt;/strong>&lt;/a> &amp;ndash; builds generating unreproducible behaviours or random artefacts;&lt;/li>
&lt;li>&lt;a href="../../06/flaky-builds">&lt;strong>Flaky builds&lt;/strong>&lt;/a> &amp;ndash; builds generating unreproducible behaviours or random artefacts;&lt;/li>
&lt;li>&lt;a href="../too-much-of-a-good-thing">&lt;em>Too much of a good thing&lt;/em>&lt;/a> &amp;ndash; when you go too far to avoid the above mistakes, causing the fix to backfire;&lt;/li>
&lt;li>&lt;a href="../implicit-assumption">&lt;em>Implicit assumption&lt;/em>&lt;/a> &amp;ndash; any build that breaks when some undocumented environmental condition change;&lt;/li>
&lt;li>&lt;a href="../untamed-plugins">&lt;em>Untamed plugins&lt;/em>&lt;/a> &amp;ndash; similar to the previous one, it is the nightmare of people that manage your build environments, when the build software uses too many, or even conflicting plugins.&lt;/li>
&lt;/ol>
&lt;p>The list is not really complete: there is one more to add.&lt;/p>
&lt;h2 id="the-unforgivable-sin-having-no-pipeline">The unforgivable sin: having no pipeline&lt;/h2>
&lt;p>This is the ultimate sin of any developer: having no automated process of any kind.&lt;br>
At a minimum you can have a simple bash or PowerShell script to compile and publish your project.
With that it is going to be easy to integrate it in any of the most popular Continuous Integration tools: Jenkins, Azure DevOps, GitHub Actions, GitLab, TeamCity, etc.&lt;br>
That script should check for dependencies and label the produced artifacts with a version number. This will be discussed in detail in future posts.&lt;br>
If you do not need to automate the process for other people, at least, automate it for your future self!&lt;/p>
&lt;p>See you soon with the next episode.&lt;/p></description></item><item><title>Code dependencies: Binary Composition is not only a mathematics calculation</title><link>http://www.getlatestversion.eu/2020/05/code-dependencies-binary-composition-is-not-only-a-mathematics-calculation/</link><pubDate>Sun, 17 May 2020 16:00:00 +0200</pubDate><author>Bartolomeo Lombardi</author><guid>http://www.getlatestversion.eu/2020/05/code-dependencies-binary-composition-is-not-only-a-mathematics-calculation/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Over the years, the growing code base of application software with multiple teams working on the same product, has lead to break up the solution into multiple solutions. This has been done trying to reduce the time required for the build and for their integration, to ease Integrated Development Environment in opening hundreds of projects, and other.
The main consequence of having multiple solutions is binary composition.&lt;/p>
&lt;h1 id="what-is-binary-composition">What is binary composition?&lt;/h1>
&lt;p>Binary composition occurs when one or more solutions reference the compiled binaries of another solution. Let suppose it is needed to make the binaries of Solution B available to Solution A before Solution A can build successfully.&lt;/p>
&lt;p>&lt;img src="visual-studio-references.jpg" alt="image.png">&lt;/p>
&lt;p>If you are forced to commit the code together with the binaries produced by build in the repository&amp;hellip;Keep on reading :)&lt;/p>
&lt;h1 id="how-can-we-distribute-binaries">How can we distribute binaries?&lt;/h1>
&lt;p>We can make those binaries available in several ways.&lt;/p>
&lt;h2 id="git-repository">Git repository&lt;/h2>
&lt;p>A first possibility consists in committing them into a repository anytime a merge into the development/master branch by means of the Continuous Integration build pipeline is requested by a Pull Request. Of course, this increases the size of the repository introducing significant slowdown checkout times and performances. Imagine what could happen if teams work on different branches ending up using different versions of the same binaries creating merge conflicts.&lt;/p>
&lt;h2 id="file-share">File share&lt;/h2>
&lt;p>Another option consists in putting the binaries onto a file share. In this case, however, there is no index to find binaries quickly and there is no protection against overriding a version.&lt;/p>
&lt;h2 id="package-management-with-azure-artifacts">Package Management with Azure Artifacts&lt;/h2>
&lt;p>This should definitely be the most suitable solution because it allows putting binaries into NuGet (and other as npm, Maven, Python, and universal) packages making it possible for Solution A to reference these packages. Among the several advantages introduced by this methodology, in Continuous Integration Azure pipeline a NuGet published task can be added in order to make the update versioning procedure automatic and distributing it in a reliable way also.&lt;/p></description></item><item><title>Reducing the gap between operations and development using Azure DevOps</title><link>http://www.getlatestversion.eu/2020/05/reducing-the-gap-between-operations-and-development-using-azure-devops/</link><pubDate>Fri, 15 May 2020 11:30:00 +0200</pubDate><author>Alessandro Alpi</author><guid>http://www.getlatestversion.eu/2020/05/reducing-the-gap-between-operations-and-development-using-azure-devops/</guid><description>&lt;h1 id="intro">Intro&lt;/h1>
&lt;p>As we all know, DevOps is culture. In a company that is going to adopt its practices and principles, everyone should be &amp;ldquo;on the same side&amp;rdquo;. Continuous improvement cannot be part of our work in IT if we wouldn&amp;rsquo;t reduce the gap between development and operations. People like me, that worked in the 90s, know that dev and ops were always isolated in silos and this was &amp;ldquo;the only&amp;rdquo; way that everyone followed as a suggestion taken from the market leaders. Ticketing systems and extreme bureaucracy acted as a man-in-the-middle between those silos, transforming each organization in two people-unaware endpoints.&lt;/p>
&lt;p>Speaking from a DevOps perspective, in such circumstances, a developer couldn&amp;rsquo;t know anything about how to deploy, where and how is an environment configured. On the other hand, an operation guy couldn&amp;rsquo;t get anything from a package in terms of what the application has been made for. Due to this gap we often see performance issues, underestimated hardware stuff and delayed releases. Last but not least, what about the lack of commitment and accountability of the employees working on the solution? In short, reducing such a gap is possible using a combination of DevOps culture and the right tools. We will see hereafter how my organization tries to do so using Azure DevOps.&lt;/p>
&lt;h2 id="scenario">Scenario&lt;/h2>
&lt;p>Let&amp;rsquo;s get started with our team (DevTeam hereafter), which is working with agile methodologies, composed of ten developers and a PO. A quick note, we are using a process decision framework called &lt;a href="https://www.disciplinedagileconsortium.org">Disciplined Agile&lt;/a>. Then, we have three operation professionals (OpsTeam from now on). Build and deploy pipelines already exist. Builds are hosted by &lt;a href="https://azure.microsoft.com/it-it/services/devops/">Azure DevOps (cloud)&lt;/a> and deploys are managed by &lt;a href="https://octopus.com/">Octopus Deploy&lt;/a>. Getting this, has been a difficult mission.&lt;/p>
&lt;p>Everything is related to infrastructure in terms of servers, operative systems, virtual hosts, DNS, firewalls, security and so on, is the responsibility of our OpsTeam. As a result, they do many tasks for managing the environments. Here comes the problem. DevTeams used to create tasks in a dedicated backlog, but OpsTeam didn&amp;rsquo;t. It&amp;rsquo;s not just a matter of end-of-pipeline tasks, but also tasks for their daily basis work.&lt;/p>
&lt;h2 id="our-solution">Our solution&lt;/h2>
&lt;p>Modify the tool, adapting its shape in order to fit in with that real scenario. A piece of cake, when you&amp;rsquo;re DevOps &amp;ldquo;inside&amp;rdquo;. How did we change Azure DevOps? Let&amp;rsquo;s describe what we did in three parts:&lt;/p>
&lt;h3 id="teams-in-azure-devops">Teams in Azure DevOps&lt;/h3>
&lt;p>To create a team in Azure DevOps is really a piece of cake (according to the latest releases). Just navigate to the options and select &lt;em>Teams&lt;/em>:&lt;/p>
&lt;p>&lt;img src="post01-01-team-on-azuredevops.png" alt="image.png">&lt;/p>
&lt;p>We can add many teams clicking on &lt;em>New team&lt;/em>:&lt;/p>
&lt;p>&lt;img src="post01-02-new-team.png" alt="image.png">&lt;/p>
&lt;p>We can set the team&amp;rsquo;s administrators, the permission set and an area under which every work item will be created. This area will be one of our best friends, especially when we will make our queries for gathering and analyzing the team&amp;rsquo;s related data. Additionally, also the other teams&amp;rsquo; members could create items with this area in order to make the OpsTeam aware of them.&lt;/p>
&lt;p>Team&amp;rsquo;s backlog
Now let&amp;rsquo;s navigate to Backlogs:&lt;/p>
&lt;p>&lt;img src="post01-03-backlogs.png" alt="image.png">&lt;/p>
&lt;p>Good! The new backlog has been created. Going on it, we will see the team&amp;rsquo;s drop-down as well as the one for iterations. Great feature!&lt;/p>
&lt;p>&lt;img src="post01-04-sprints.png" alt="image.png">&lt;/p>
&lt;p>Once created, we will see the teams&amp;rsquo; drop-down:&lt;/p>
&lt;p>&lt;img src="post01-05-team-switch.png" alt="image.png">&lt;/p>
&lt;h3 id="work-items">Work items&lt;/h3>
&lt;p>Now, let&amp;rsquo;s create a new work item type. We call it Ops item. Navigate to Process customization page:&lt;/p>
&lt;p>&lt;img src="post01-06-process.png" alt="image.png">&lt;/p>
&lt;p>Before adding the new work item, we must ensure that the process is already a custom process, otherwise, all the edits will be blocked as shown in the following picture:&lt;/p>
&lt;p>&lt;img src="post01-07-process-custom.png" alt="image.png">&lt;/p>
&lt;p>We&amp;rsquo;ve already created a SimplifiedScrum process. let&amp;rsquo;s add our item now:&lt;/p>
&lt;p>&lt;img src="post01-07-workitemtype.png" alt="image.png">&lt;/p>
&lt;p>Now we are going to modify the fields of the new type. Each team should be able to understand all the item&amp;rsquo;s properties. We will leave the item as is in this example. Then, we have to map the type to the backlog item list, since only the default work item types are shown initially. To do this, navigate to the Process customization page, Backlog Levels tab:&lt;/p>
&lt;p>&lt;img src="post01-08-process-add-workitemtype-1.png" alt="image.png">
&lt;img src="post01-09-process-add-workitemtype-2.png" alt="image.png">&lt;/p>
&lt;p>As we can see, we can also set the default item for our requirements backlog. Moreover, every Sprint backlog, based on iterations, will enable us to add the new Ops item:&lt;/p>
&lt;p>&lt;img src="post01-10-new-workitem.png" alt="image.png">&lt;/p>
&lt;h3 id="wrapping-up">Wrapping up&lt;/h3>
&lt;p>So, we&amp;rsquo;ve got a backlog for the IT Operations team&amp;rsquo;s members. Then, we&amp;rsquo;ve related it to their Azure DevOps team. Additionally, we&amp;rsquo;ve got a particular work item type (not mandatory, but really useful for querying it or adding it into dashboards) target of IT Operations&amp;rsquo; job and a dedicated Area Path. We can make many relationships between items of both our backlogs. Here is an example of how an activity can be managed and organized (extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-devlabs.WorkItemVisualization">Work Item Visualize&lt;/a>):&lt;/p>
&lt;p>&lt;img src="post01-11-workitem-visualize.png" alt="image.png">&lt;/p>
&lt;p>As you can see, the Ops items are &lt;em>Successor&lt;/em> of the &amp;ldquo;development&amp;rdquo; Product backlog items. Indeed, the Ops Items will be finished after the PBI, typically. Think about creating s DNS or a network path to let the production app work in production.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>With this solution, we&amp;rsquo;re decoupling backlogs. Moreover, we&amp;rsquo;re isolating the management maintaining the relationships between work items that reside on different boards. At the same time, we&amp;rsquo;re making a strong synergy between Development and Operations.
Then, in a couple of clicks, we can switch teams and backlogs, using Azure DevOps Boards. We can track changes in both the departments, also for audit requirements. In my opinion, this helps the enterprise awareness and facilitates the continuous improvement of all the teams and any member&amp;rsquo;s skill.&lt;/p></description></item><item><title>About</title><link>http://www.getlatestversion.eu/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://www.getlatestversion.eu/about/</guid><description>&lt;p>&lt;img src="http://www.getlatestversion.eu/images/320px-Flag_of_Europe.svg.png" alt="Europe flag">&lt;/p>
&lt;h2 id="about-us">About Us&lt;/h2>
&lt;p>GetLatestVersion.eu stems from GetLatestVersion.it, the successful Italian community specialised on DevOps, Application Lifecycle Management (ALM) and Software Development Life-Cycle (SDLC).&lt;/p>
&lt;h2 id="what-we-do">What we do&lt;/h2>
&lt;p>We publish posts and articles on DevOps, Agile, tools, design, implementation techniques.
Our &lt;a href="https://www.youtube.com/GetLatestVersion">YouTube channel&lt;/a> hosts useful video of different kind: short introductions to a topic or tool, longer presentation on experiences and conference sessions.
In addition, we organise events in person and online on DevOps, Agile, ALM and SDLC topics.&lt;/p></description></item><item><title>Privacy</title><link>http://www.getlatestversion.eu/page/privacy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://www.getlatestversion.eu/page/privacy/</guid><description>&lt;h2 id="list-of-cookies-used-in-this-site">List of Cookies used in this site&lt;/h2>
&lt;p>&lt;strong>Google Analytics cookies&lt;/strong>: __utma, __utmb, __utmc, __utmv, __utmz, used to gather site statistic usage. You can read more info on Google Policies on: How Google uses data when you use our partners’ sites or apps and Safeguarding your Data.&lt;/p></description></item><item><title>Search</title><link>http://www.getlatestversion.eu/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://www.getlatestversion.eu/search/</guid><description>&lt;p class="error message js-hidden">You must have Javascript enabled to use this function.&lt;/p>
&lt;p class="search-loading status message hidden">Loading search index…&lt;/p>
&lt;div class="search-input hidden">
&lt;form id="search-form" class="search-form" action="#" method="post" accept-charset="UTF-8" role="search">
&lt;label for="query" class="visually-hidden">Search&lt;/label>
&lt;input type="search" id="query" name="query" class="search-text" placeholder="Enter the terms you wish to search for." maxlength="128">
&lt;button type="submit" name="submit" class="form-submit" >Search&lt;/button>
&lt;/form>
&lt;/div>
&lt;div class="search-results">&lt;/div>
&lt;template>
&lt;article class="search-result list-view">
&lt;header>
&lt;h2 class="title title-submitted">&lt;a href="#">Title here&lt;/a>&lt;/h2>
&lt;div class="submitted">&lt;time class="created-date">Date here&lt;/time>&lt;/div>
&lt;/header>
&lt;div class="content">Summary here&lt;/div>
&lt;/article>
&lt;/template></description></item></channel></rss>